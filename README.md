# CMIP6\_Climate\_Classification\_IvanFernandez\_SergioAlves

Este repositorio (`ProyectoOsos`) contiene un conjunto de scripts de Python dise√±ados para procesar datos de modelos clim√°ticos (como CMIP6) y clasificarlos en zonas clim√°ticas o h√°bitats distintos mediante t√©cnicas de Machine Learning no supervisado (PCA y K-Means).

El pipeline completo realiza las siguientes operaciones:

1.  **Verificaci√≥n de Datos:** Comprueba la consistencia de los archivos de entrada (grid, unidades).
2.  **Preprocesamiento:** Remalla (regridding) los datos a una grid com√∫n aplicando una m√°scara de tierra.
3.  **Agregaci√≥n:** Une los datos por modelo, calcula las climatolog√≠as mensuales y crea un *ensemble* (promedio) multi-modelo.
4.  **Reducci√≥n de Dimensionalidad:** Aplica An√°lisis de Componentes Principales (PCA) sobre las variables clim√°ticas (`pr`, `tasmax`, `tasmin`).
5.  **Clustering:**
      * Calcula el n√∫mero √≥ptimo de cl√∫steres (`k`) usando el "M√©todo del Codo".
      * Genera un mapa de clasificaci√≥n global usando K-Means con el `k` √≥ptimo.
6.  **An√°lisis de H√°bitats:** Identifica los cl√∫steres resultantes con h√°bitats de inter√©s (ej. Oso Polar, Panda) bas√°ndose en puntos de muestra geogr√°ficos.

-----

## üìÇ Estructura de Carpetas y Descarga de Datos

Para que los scripts funcionen correctamente, se espera la siguiente estructura de directorios (los scripts se encuentran en `ProyectoOsos/scripts/`):

```
ProyectoOsos/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ instalacion_data.txt  (CONTIENE ENLACE GOOGLE DRIVE)
‚îÇ   ‚îú‚îÄ‚îÄ pr/
‚îÇ   ‚îú‚îÄ‚îÄ tasmax/
‚îÇ   ‚îî‚îÄ‚îÄ tasmin/
‚îú‚îÄ‚îÄ data_auxiliar/
‚îÇ   ‚îî‚îÄ‚îÄ landsea.nc
‚îú‚îÄ‚îÄ data_climatologia/
‚îú‚îÄ‚îÄ data_ensemble/
‚îú‚îÄ‚îÄ data_kmeans/
‚îú‚îÄ‚îÄ data_pca/
‚îú‚îÄ‚îÄ data_remallada/
‚îÇ   ‚îî‚îÄ‚îÄ instalacion_data_remallada.txt (CONTIENE ENLACE)
‚îú‚îÄ‚îÄ data_unida/
‚îÇ   ‚îî‚îÄ‚îÄ instalacion_data_unida.txt (CONTIENE ENLACE)
‚îú‚îÄ‚îÄ figures/
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ ... (todos los scripts .py)
```

### ‚ùó Nota Importante sobre los Datos (Google Drive)

Debido al gran tama√±o de los archivos NetCDF (`.nc`) iniciales, el **contenido** de las carpetas `data`, `data_remallada` y `data_unida` no est√° alojado en GitHub.

En su lugar, dentro de cada una de estas tres carpetas en el repositorio, encontrar√°s un archivo `.txt` (ej. `instalacion_data.txt`) que contiene un enlace de Google Drive.

**Instrucciones de descarga:**

1.  Navega a la carpeta correspondiente en este repositorio (ej. `ProyectoOsos/data/`).
2.  Abre el archivo `.txt` que se encuentra dentro.
3.  Copia el enlace de Google Drive y √∫salo para descargar el archivo comprimido.
4.  Descomprime el contenido descargado dentro de esa misma carpeta para que el pipeline pueda encontrar los archivos (`.nc`).

Las carpetas de datos generados *despu√©s* del preprocesamiento (`data_climatologia`, `data_ensemble`, etc.) son mucho m√°s peque√±as y s√≠ est√°n incluidas directamente en el repositorio.

-----

## üì¶ Instalaci√≥n

Se recomienda crear un entorno virtual (por ejemplo, con `conda` o `venv`) e instalar las dependencias.

```bash
pip install -r requirements.txt
```

El archivo `requirements.txt` deber√≠a contener, como m√≠nimo:

```
numpy
pandas
xarray
scikit-learn
kneed
matplotlib
cartopy
joblib
```

-----

## üöÄ Recomendaci√≥n de Ejecuci√≥n

Aunque en el repositorio incluimos las carpetas de datos intermedios (`data_climatologia`, `data_ensemble`, etc.) resultantes de nuestra propia ejecuci√≥n, **recomendamos ejecutar todo el pipeline desde el principio**.

Para ello, una vez descargados los datos iniciales (`data`, `data_remallada`, `data_unida` y `data_auxiliar`) siguiendo las instrucciones de la nota anterior, solo tienes que seguir el "Flujo de Ejecuci√≥n" descrito a continuaci√≥n para regenerar todos los resultados.

-----

## ‚öôÔ∏è Flujo de Ejecuci√≥n (Workflow)

Los scripts deben ejecutarse en un orden espec√≠fico para que el pipeline funcione. **(Ejecutar desde la carpeta `ProyectoOsos/scripts/`)**

### Parte 1: Preprocesamiento de Datos

Se procesan las variables `pr`, `tasmax` y `tasmin` desde los datos crudos hasta un *ensemble* listo para el an√°lisis.

```bash
# 1. Verificar consistencia de los datos originales (Opcional pero recomendado)
python verificar_datos_originales_todas_las_variables.py

# 2. Remallar todos los datos a una grid fija (usando data_auxiliar/landsea.nc)
python remallar_a_grid_fijo_todas_las_variables.py

# 3. Unir los archivos remallados (series temporales) por modelo
python unir_remallados_por_modelo_todas_las_variables.py

# 4. Calcular las climatolog√≠as mensuales (promedio de 12 meses)
python calcular_climatologias_todas_las_variables.py

# 5. Crear el ensemble multi-modelo (promedio de todos los modelos)
python crear_ensemble_todas_las_variables.py
```

### Parte 2: An√°lisis de Machine Learning (Autom√°tico)

Este flujo utiliza el "M√©todo del Codo" para determinar autom√°ticamente el mejor n√∫mero de cl√∫steres (`k`).

```bash
# 6. Aplicar PCA sobre los datos del ensemble
python aplicar_pca.py

# 7. Calcular el 'k' √≥ptimo con el M√©todo del Codo y guardarlo
python calcular_y_guardar_codo.py

# 8. Generar el mapa K-Means usando el 'k' √≥ptimo guardado
python generar_mapa_kmeans.py

# 9. Analizar el mapa K-Means e identificar los h√°bitats
python analizar_y_mapear_habitats_pandaversion.py
```

### üìà Resultados (Workflow Autom√°tico)

  * `../figures/metodo_del_codo.png`: Gr√°fico del M√©todo del Codo.
  * `../data_kmeans/k_optimo.txt`: Archivo de texto con el `k` √≥ptimo detectado.
  * `../data_kmeans/mapa_clasificacion_k[N].nc`: Dataset NetCDF con la clasificaci√≥n.
  * `../figures/mapa_clasificacion_k[N].png`: Mapa global de las zonas clim√°ticas.
  * `../figures/mapa_clusters_habitats.png`: Mapa final con los h√°bitats identificados.

-----

## üó∫Ô∏è Flujo Alternativo (Selecci√≥n Manual de `k`)

Si prefieres forzar un n√∫mero espec√≠fico de cl√∫steres (por ejemplo, `k=9`) e ignorar el M√©todo del Codo, puedes usar los scripts alternativos.

**Sigue la Parte 1 (pasos 1-5) y el paso 6 (PCA).** Luego, en lugar de los pasos 7 y 8:

```bash
# 7. (Alternativo) Generar el mapa forzando k=9
python nueve_clusters.py

# 8. (Alternativo) Generar el mapa forzando k=5
python cinco_clusters.py

# ... (Igualmente para 7, 8 o 10 clusters)

# 9. Analizar el mapa generado
# (Este script detectar√° autom√°ticamente el √∫ltimo mapa creado)
python analizar_y_mapear_habitats_pandaversion.py
```

-----

## üìú Descripci√≥n de Scripts

(Rutas relativas asumidas desde la carpeta `scripts/`)

  * `verificar_datos_originales_...`: Lee `../data/` y comprueba la consistencia de grids y unidades antes de procesar.
  * `remallar_a_grid_fijo_...`: Estandariza la resoluci√≥n espacial de todos los modelos a una grid com√∫n (64x128) y aplica la m√°scara `../data_auxiliar/landsea.nc`. Guarda en `../data_remallada/`.
  * `unir_remallados_por_modelo_...`: Concatena las series temporales de cada modelo. Guarda en `../data_unida/`.
  * `calcular_climatologias_...`: Calcula la media mensual para cada modelo. Guarda en `../data_climatologia/`.
  * `crear_ensemble_...`: Calcula la media de todos los modelos, creando el archivo final para el an√°lisis. Guarda en `../data_ensemble/`.
  * `aplicar_pca.py`: Carga los datos del ensemble, los estandariza y aplica PCA. Guarda los componentes principales (CPs) en `../data_pca/componentes_principales.nc`.
  * `calcular_y_guardar_codo.py`: Ejecuta K-Means para un rango de `k` (2 a 20), genera el gr√°fico del codo (`../figures/`) y guarda el `k` √≥ptimo en `../data_kmeans/k_optimo.txt`.
  * `generar_mapa_kmeans.py`: Lee `../data_kmeans/k_optimo.txt`, entrena el modelo K-Means final con ese `k` y guarda el mapa NetCDF y PNG.
  * `(cinco|siete|ocho|nueve|diez)_clusters.py`: Variantes de `generar_mapa_kmeans.py` que fuerzan un valor `k` manual (5, 7, 8, 9 o 10).
  * `analizar_y_mapear_habitats_...`: Script final. Carga el mapa K-Means m√°s reciente de `../data_kmeans/`, usa puntos de muestra (ej. "Oso Polar", "Oso Pardo") para identificar a qu√© cl√∫ster pertenecen, y genera el mapa final de h√°bitats en `../figures/`.
